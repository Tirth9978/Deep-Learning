{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfmZrk-fYE3Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5CqdW2JYBhB"
      },
      "source": [
        "## ACTIVATION FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqNZ1pWWX_je",
        "outputId": "ecc3341c-c1a6-4a7a-a776-d39d3fe18317"
      },
      "outputs": [],
      "source": [
        "class ActivationFunctions:\n",
        "    \"\"\" Let's implement all activation functions manually \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(z):\n",
        "        return 1 / (1 + torch.exp(-z))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(z):\n",
        "        s = ActivationFunctions.sigmoid(z)\n",
        "        return s * (1 - s)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(z):\n",
        "        return torch.tanh(z)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(z):\n",
        "        return 1 - torch.tanh(z)**2\n",
        "\n",
        "    @staticmethod\n",
        "    def relu(z):\n",
        "        return torch.maximum(torch.tensor(0.0), z)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_derivative(z):\n",
        "        return torch.where(z > 0, torch.tensor(1.0), torch.tensor(0.0))\n",
        "\n",
        "\n",
        "def test_activations():\n",
        "    print(\"ðŸ§ª Testing Activation Functions:\")\n",
        "\n",
        "    # Create test input\n",
        "    x = torch.tensor([-1.0, 0.0, 1.0, 2.0])\n",
        "\n",
        "    print(f\"Input: {x}\")\n",
        "    print(f\"Sigmoid: {ActivationFunctions.sigmoid(x)}\")\n",
        "    print(f\"Tanh: {ActivationFunctions.tanh(x)}\")\n",
        "    print(f\"ReLU: {ActivationFunctions.relu(x)}\")\n",
        "\n",
        "test_activations()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ASQzyXLYqsw"
      },
      "source": [
        "## THE PERCEPTRON (Single Neuron)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWVDlhThYrVF",
        "outputId": "446d7493-ca2b-4573-924b-62c5c2fc5ede"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "class SingleNeuron(nn.Module) :\n",
        "    def __init__(self,input_Size):\n",
        "        super().__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(input_Size))\n",
        "        self.bias = nn.Parameter(torch.randn(1))\n",
        "\n",
        "    def forward(self , X):\n",
        "        z = torch.dot(X , self.weights) + self.bias\n",
        "\n",
        "        y = torch.sigmoid(z)\n",
        "        return z\n",
        "\n",
        "\n",
        "print(\"Creating the SingleNeuron :)\")\n",
        "PERCEPTRON = SingleNeuron(input_Size=4)\n",
        "print(f\"Weights: {PERCEPTRON.weights}\")\n",
        "print(f\"Bias: {PERCEPTRON.bias}\")\n",
        "\n",
        "input = torch.tensor([0.2,0.4,0.120, 0.999])\n",
        "\n",
        "print(\"Output : \")\n",
        "PERCEPTRON(X = input)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
