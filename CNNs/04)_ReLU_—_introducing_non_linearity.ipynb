{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ReLU ‚Äî introducing non-linearity\n",
        "A CNN without ReLU is just a big linear equation üéØ\n",
        "ReLU is what allows CNNs to learn complex patterns."
      ],
      "metadata": {
        "id": "hRFsxoMRLGrf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cMyUMHlpKiWx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1, 1, 5, 5)\n",
        "\n",
        "conv = nn.Conv2d(1,1,kernel_size= 3 , padding =1 )\n",
        "relu = nn.ReLU() # This Creates the non - Linearity\n",
        "\n",
        "y = relu(conv(x))\n",
        "\n",
        "print(y.min() , y.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMDO4GMjKqeK",
        "outputId": "cf2c5f45-6fe0-4f4c-a015-cbac10f5ccea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0., grad_fn=<MinBackward1>) tensor(0.7406, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Line-by-Line Explanation\n",
        "`conv = nn.Conv2d(1, 1, kernel_size=3, padding=1)`\n",
        "- Extract the Features\n",
        "- Output contains positive and negative values\n",
        "CNNs alone do not decide importance ‚ùå"
      ],
      "metadata": {
        "id": "i-R1JrvwLLHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why ReLU (and not sigmoid/tanh)\n",
        "| Activation | Problem              |\n",
        "| ---------- | -------------------- |\n",
        "| Sigmoid    | Vanishing gradients  |\n",
        "| Tanh       | Slow training        |\n",
        "| ReLU       | Fast, stable, simple |\n",
        "\n",
        "ReLU made deep CNNs possible üöÄ"
      ],
      "metadata": {
        "id": "h9G-w3TGLmR3"
      }
    }
  ]
}